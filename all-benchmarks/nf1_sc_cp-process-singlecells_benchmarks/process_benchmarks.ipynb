{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Benchmark Processing with NF1-dataset using CytoSnake's `cp_process_singlecells` Workflow\n",
    "\n",
    "This notebook uses the benchmark data obtained through the application of CytoSnake's `cp_process_singlecells` workflow. The collected data is then compiled into a unified benchmark profile, allowing comparisons with other generated benchmark profiles in this repo.\n",
    "\n",
    "This benchmark profile was generated  by using the [`NF1-data`](https://github.com/WayScience/nf1_cellpainting_data)\n",
    "\n",
    "**NOTE**: This benchmark profile only profiles the single-cell pipeline found [here](https://github.com/WayScience/nf1_cellpainting_data/blob/main/3.processing_features/2.pycytominer_singlecell_pipelines.ipynb).\n",
    "\n",
    "The bulk benchmarking profile will be found in the `all-benchmarks/nf1_bulk_cp-process-singlecells_benchmarks` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pathlib\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "from src.benchmark_utils import get_benchmark_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating CytoSnake `cp_process_singlecells` Benchmark Profile with NF1 Data\n",
    "\n",
    "In this section, we are generating a benchmark profile for CytoSnake's `cp_process_singlecells` applied to the NF1 dataset. For additional information on the NF1 dataset, detailed documentation is available in its repository and can also be found in the controls directory of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "CWD_PATH = pathlib.Path(\".\").resolve()\n",
    "BENCHMARK_DIR = pathlib.Path(\"./benchmarks/\").resolve(strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Plate_1': 4.727, 'Plate_3': 551.629, 'Plate_3_prime': 444.938, 'Plate_4': 222.945, 'Plate_2': 30.598}\n"
     ]
    }
   ],
   "source": [
    "# loading in file size data:\n",
    "with open(\"./file_size.json\", mode=\"r\") as stream:\n",
    "    file_sizes = json.load(stream)\n",
    "\n",
    "# this is in MB\n",
    "print(file_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmarks/Plate_3_nf1_feature_select_benchmarks.bin was successfully converted into benchmarks/Plate_3_nf1_feature_select_benchmarks.json\n",
      "benchmarks/Plate_3_prime_nf1_analysis_normalize_benchmarks.bin was successfully converted into benchmarks/Plate_3_prime_nf1_analysis_normalize_benchmarks.json\n",
      "benchmarks/Plate_4_nf1__feature_select_benchmarks.bin was successfully converted into benchmarks/Plate_4_nf1__feature_select_benchmarks.json\n",
      "benchmarks/Plate_2_nf1_feature_select_benchmarks.bin was successfully converted into benchmarks/Plate_2_nf1_feature_select_benchmarks.json\n",
      "benchmarks/Plate_1_nf1_analysis_normalize_benchmarks.bin was successfully converted into benchmarks/Plate_1_nf1_analysis_normalize_benchmarks.json\n",
      "benchmarks/Plate_1_nf1_feature_select_benchmarks.bin was successfully converted into benchmarks/Plate_1_nf1_feature_select_benchmarks.json\n",
      "benchmarks/feature_select_benchmarks.bin was successfully converted into benchmarks/feature_select_benchmarks.json\n",
      "benchmarks/Plate_2_nf1_analysis_annotate_benchmarks.bin was successfully converted into benchmarks/Plate_2_nf1_analysis_annotate_benchmarks.json\n",
      "benchmarks/Plate_3_prime_nf1_analysis_annotate_benchmarks.bin was successfully converted into benchmarks/Plate_3_prime_nf1_analysis_annotate_benchmarks.json\n",
      "benchmarks/Plate_4_nf1_analysis_annotate_benchmarks.bin was successfully converted into benchmarks/Plate_4_nf1_analysis_annotate_benchmarks.json\n",
      "benchmarks/Plate_3_prime_nf1_feature_select_benchmarks.bin was successfully converted into benchmarks/Plate_3_prime_nf1_feature_select_benchmarks.json\n",
      "benchmarks/Plate_3_nf1_analysis_normalize_benchmarks.bin was successfully converted into benchmarks/Plate_3_nf1_analysis_normalize_benchmarks.json\n",
      "benchmarks/Plate_4_nf1_analysis_normalize_benchmarks.bin was successfully converted into benchmarks/Plate_4_nf1_analysis_normalize_benchmarks.json\n",
      "benchmarks/Plate_1_nf1_analysis_annotate_benchmarks.bin was successfully converted into benchmarks/Plate_1_nf1_analysis_annotate_benchmarks.json\n",
      "benchmarks/Plate_3_nf1_analysis_annotate_benchmarks.bin was successfully converted into benchmarks/Plate_3_nf1_analysis_annotate_benchmarks.json\n",
      "benchmarks/Plate_2_nf1_analysis_normalize_benchmarks.bin was successfully converted into benchmarks/Plate_2_nf1_analysis_normalize_benchmarks.json\n"
     ]
    }
   ],
   "source": [
    "# converting .bin files into .json files (~2min)\n",
    "# skip this block if the json files are already in the benchmarks/ directory\n",
    "for bin_path in get_benchmark_files(BENCHMARK_DIR, ext=\"bin\"):\n",
    "    json_out = BENCHMARK_DIR / f\"{bin_path.stem}.json\"\n",
    "\n",
    "    # # executing memray to convert bin files into json files\n",
    "    memray_stats = subprocess.run(\n",
    "        [\n",
    "            \"memray\",\n",
    "            \"stats\",\n",
    "            \"--json\",\n",
    "            \"--output\",\n",
    "            str(json_out),\n",
    "            \"--force\",\n",
    "            str(bin_path),\n",
    "        ],\n",
    "        capture_output=True,\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "    # stdout message\n",
    "    print(\n",
    "        f\"{bin_path.relative_to(CWD_PATH)} was successfully converted into {json_out.relative_to(CWD_PATH)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# accessing to all metadata from benchmarks\u001b[39;00m\n\u001b[1;32m     20\u001b[0m meta_data \u001b[38;5;241m=\u001b[39m benchmark_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     21\u001b[0m selected_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpid\u001b[39m\u001b[38;5;124m\"\u001b[39m: meta_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpid\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: process_name,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_data_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: plate_name,\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m\"\u001b[39m: datetime\u001b[38;5;241m.\u001b[39mstrptime(meta_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m\"\u001b[39m], tformat),\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m\"\u001b[39m: datetime\u001b[38;5;241m.\u001b[39mstrptime(meta_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m\"\u001b[39m], tformat),\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_duration\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\n\u001b[1;32m     28\u001b[0m         datetime\u001b[38;5;241m.\u001b[39mstrptime(meta_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m\"\u001b[39m], tformat)\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;241m-\u001b[39m datetime\u001b[38;5;241m.\u001b[39mstrptime(meta_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m\"\u001b[39m], tformat)\n\u001b[1;32m     30\u001b[0m     )\u001b[38;5;241m.\u001b[39mtotal_seconds(),\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_allocations\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(meta_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_allocations\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeak_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mint\u001b[39m(meta_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeak_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: file_sizes[plate_name],\n\u001b[1;32m     34\u001b[0m }\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# append to list\u001b[39;00m\n\u001b[1;32m     36\u001b[0m raw_benchmark_data\u001b[38;5;241m.\u001b[39mappend(selected_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'process_name' is not defined"
     ]
    }
   ],
   "source": [
    "# using all json files to compile benchmark profile\n",
    "raw_benchmark_data = []\n",
    "for json_path in get_benchmark_files(BENCHMARK_DIR, ext=\"json\"):\n",
    "    # collecting data from just file name\n",
    "    plate_name = json_path.stem.split(\"_nf1_\")[0]\n",
    "    try:\n",
    "        file_size = file_sizes[plate_name]\n",
    "    except KeyError:\n",
    "        file_size = \"0\"\n",
    "\n",
    "    # opening json file to extract benchmark information\n",
    "    with open(json_path, encoding=\"utf-8\", mode=\"r\") as contents:\n",
    "        benchmark_data = json.load(contents)\n",
    "\n",
    "        # accessing to all metadata from benchmarks\n",
    "        meta_data = benchmark_data[\"metadata\"]\n",
    "        selected_data = {\n",
    "            \"pid\": meta_data[\"pid\"],\n",
    "            \"process_name\": process_name,\n",
    "            \"input_data_name\": plate_name,\n",
    "            \"start_time\": datetime.strptime(meta_data[\"start_time\"], tformat),\n",
    "            \"end_time\": datetime.strptime(meta_data[\"end_time\"], tformat),\n",
    "            \"time_duration\": (\n",
    "                datetime.strptime(meta_data[\"end_time\"], tformat)\n",
    "                - datetime.strptime(meta_data[\"start_time\"], tformat)\n",
    "            ).total_seconds(),\n",
    "            \"total_allocations\": int(meta_data[\"total_allocations\"]),\n",
    "            \"peak_memory\": round(int(meta_data[\"peak_memory\"]) / 1024**2, 3),\n",
    "            \"file_size\": file_sizes[plate_name],\n",
    "        }\n",
    "        # append to list\n",
    "        raw_benchmark_data.append(selected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating benchmark profile and saving it\n",
    "benchmark_profile = pd.DataFrame(data=raw_benchmark_data)\n",
    "benchmark_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cytosnake_benchmarks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
